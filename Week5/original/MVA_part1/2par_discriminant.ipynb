{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-parameters discriminant analysis\n",
    "\n",
    "Python notebook for constructing a Fisher disciminant from two 2D Gaussianly distributed correlated variables. The notebook creates artificial random data for two different types of processes, and the goal is then to separate these by constructing a Fisher discriminant.\n",
    "\n",
    "### Authors: \n",
    "- Christian Michelsen (Niels Bohr Institute)\n",
    "- Troels C. Petersen (Niels Bohr Institute)\n",
    "\n",
    "### Date:    \n",
    "- 15-12-2021 (latest update)\n",
    "\n",
    "### References:\n",
    "- Glen Cowan, Statistical Data Analysis, pages 51-57\n",
    "- http://en.wikipedia.org/wiki/Linear_discriminant_analysis\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                                     # Matlab like syntax for linear algebra and functions\n",
    "import matplotlib.pyplot as plt                        # Plots and figures like you know them from Matlab\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.random             # Random generator\n",
    "r.seed(42)                # Set a random seed (but a fixed one)\n",
    "save_plots = False          # For now, don't save plots (once you trust your code, switch on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions:\n",
    "\n",
    "Function to calculate the separation betweem two lists of numbers (see equation at the bottom of the script).\n",
    "\n",
    "__Note__: You need to fill in this function! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_separation(x, y):\n",
    "    \n",
    "    print(\"calc_separation needs to be filled out\")\n",
    "    \n",
    "    d = 0\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define parameters:\n",
    "\n",
    "\n",
    "Number of species, their means and widths, correlations and the number of observations of each species:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of 'species': signal / background\n",
    "n_spec = 2       \n",
    "\n",
    "# Species A, mean and width for the two dimensions/parameters\n",
    "mean_A  = [15.0, 50.0] \n",
    "width_A = [ 2.0,  6.0] \n",
    "\n",
    "# Species B, mean and width for the two dimensions/parameters\n",
    "mean_B  = [12.0, 55.0] \n",
    "width_B = [ 3.0,  6.0] \n",
    "\n",
    "# Coefficient of correlation\n",
    "corr_A = 0.8\n",
    "corr_B = 0.9\n",
    "\n",
    "# Amount of data you want to create\n",
    "n_data = 2000         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate data:\n",
    "\n",
    "For each \"species\", produce a number of $(x_0,x_1)$ points which are (linearly) correlated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# The desired covariance matrix.\n",
    "V_A = np.array([[width_A[0]**2, width_A[0]*width_A[1]*corr_A],\n",
    "                [width_A[0]*width_A[1]*corr_A, width_A[1]**2]])\n",
    "V_B = np.array([[width_B[0]**2, width_B[0]*width_B[1]*corr_B],\n",
    "                [width_B[0]*width_B[1]*corr_B, width_B[1]**2]])\n",
    "\n",
    "# Generate the random samples.\n",
    "spec_A = np.random.multivariate_normal(mean_A, V_A, size=n_data)\n",
    "spec_B = np.random.multivariate_normal(mean_B, V_B, size=n_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Plot your generated data:\n",
    "\n",
    "We plot the 2D-data as 1D-histograms (basically projections) in $x_0$ and $x_1$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_1D, ax_1D = plt.subplots(ncols=2, figsize=(14, 6))\n",
    "\n",
    "ax_1D[0].hist(spec_A[:, 0], 50, (0, 25), histtype='step', label='Species A', color='Red', lw=1.5)\n",
    "ax_1D[0].hist(spec_B[:, 0], 50, (0, 25), histtype='step', label='Species B', color='Blue', lw=1.5)\n",
    "ax_1D[0].set(title='Parameter x0', xlabel='x0', ylabel='Counts', xlim=(0,25))\n",
    "ax_1D[0].legend(loc='upper left')\n",
    "\n",
    "# uncomment later\n",
    "#ax_1D[0].text(1, 176, fr'$\\Delta_{{x0}} = {calc_separation(spec_A[:, 0], spec_B[:, 0]):.3f}$', fontsize=16)\n",
    "\n",
    "ax_1D[1].hist(spec_A[:, 1], 50, (20, 80), histtype='step', label='Species A', color='Red', lw=1.5)\n",
    "ax_1D[1].hist(spec_B[:, 1], 50, (20, 80), histtype='step', label='Species B', color='Blue', lw=1.5)\n",
    "ax_1D[1].set(title='Parameter x1', xlabel='x1', ylabel='Counts', xlim=(20, 80))\n",
    "ax_1D[1].legend(loc='upper left')\n",
    "\n",
    "# uncomment later\n",
    "#ax_1D[1].text(22, 140, fr'$\\Delta_{{x1}} = {calc_separation(spec_A[:, 1], spec_B[:, 1]):.3f}$', fontsize=16)\n",
    "\n",
    "fig_1D.tight_layout()\n",
    "\n",
    "if save_plots :\n",
    "    fig_1D.savefig('InputVars_1D.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Wait with drawing the 2D distribution, so that you think about the 1D distributions first!\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the two 1D figures, it seems that species A and B can be separated to some degree, but not very well. If you were to somehow select cases of species A, then I can imagine a selection as follows:\n",
    " - If (x0 > 16) or (x1 < 46) or (x0 > 13 and x1 < 52), then guess / select as A.\n",
    "\n",
    "Think about this yourself, and discuss with your peers, how you would go about separating A from B based on x0 and x1.\n",
    "\n",
    " -----------------------  5-10 minutes later  -----------------------\n",
    " \n",
    "As it is, this type of selection is hard to optimise, especially with more dimensions (i.e. more variables than just x0 and x1). That is why Fisher's linear discriminant, $F$, is very useful. It makes the most separating linear combination of the input variables, and the coefficients can be calculated analytically. Thus, it is fast, efficient, and transparent. And it takes linear correlations into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig_corr, ax_corr = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "# ax_corr.scatter(spec_A[:, 0], spec_A[:, 1], color='Red',  s=10, label='Species A')\n",
    "# ax_corr.scatter(spec_B[:, 0], spec_B[:, 1], color='Blue', s=10, label='Species B')\n",
    "# ax_corr.set(xlabel='Parameter x0', ylabel='Parameter x1', title='Correlation');\n",
    "\n",
    "# ax_corr.legend();\n",
    "# fig_corr.tight_layout()\n",
    "\n",
    "#if save_plots :\n",
    "#    fig_corr.savefig('InputVars_2D.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fisher Discriminant calculation:\n",
    "\n",
    "We want to find $\\vec{w}$ defined by:\n",
    "\n",
    "$$\\vec{w} = \\left(\\Sigma_A + \\Sigma_B\\right)^{-1} \\left(\\vec{\\mu}_A - \\vec{\\mu}_B\\right)$$  \n",
    "\n",
    "which we use to project our data into the best separating plane (line in this case) given by:\n",
    "\n",
    "$$ \\mathcal{F} = w_0 + \\vec{w} \\cdot \\vec{x} $$\n",
    "\n",
    "We start by finding the means and covariance of the individuel species: (__fill in yourself!__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_A = 0 # fill in yourself\n",
    "mu_B = 0 # fill in yourself\n",
    "mu_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_A = 0 # fill in yourself\n",
    "cov_B = 0 # fill in yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_sum = cov_A + cov_B\n",
    "cov_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where `cov_sum` is the sum of the all of the species' covariance matrices. We invert this using scipy's `inv` function.  __Note__: fill in yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Delete the definition below of cov_sum when you have filled in the cells above:\n",
    "cov_sum = np.diag([1, 2])\n",
    "\n",
    "# Inverts cov_sum\n",
    "cov_sum_inv = inv(cov_sum)\n",
    "cov_sum_inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the fisher weights, $\\vec{w}$. __Note__: fill in yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wf = np.ones(2) # fill in yourself\n",
    "wf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the fisher discriminant, $\\mathcal{F}$. __Note__: fill in yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fisher_data_A = spec_A[:, 0] * (-1.4) + 10 # fill in yourself\n",
    "fisher_data_B = spec_B[:, 0] * (-1.4) + 10 # fill in yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_fisher, ax_fisher = plt.subplots(figsize=(12, 8))\n",
    "ax_fisher.hist(fisher_data_A, 200, (-22, 3), histtype='step', color='Red', label='Species A')\n",
    "ax_fisher.hist(fisher_data_B, 200, (-22, 3), histtype='step', color='Blue', label='Species B')\n",
    "ax_fisher.set(xlim=(-22, 3), xlabel='Fisher-discriminant')\n",
    "ax_fisher.legend()\n",
    "\n",
    "# ax_fisher.text(-21, 60, fr'$\\Delta_{{fisher}} = {calc_separation(fisher_data_A, fisher_data_B):.3f}$', fontsize=16)\n",
    "\n",
    "fig_fisher.tight_layout()\n",
    "\n",
    "if save_plots:\n",
    "    fig_fisher.savefig('FisherOutput.pdf', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is easy to visually see the increased seperation (when done correctly). We can also compare $\\Delta_{fisher}$ to $\\Delta_{x0}$ or $\\Delta_{x1}$ and see it clearly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Questions\n",
    "\n",
    "As always, make sure that you know what the code is doing so far, and what the aim of the exercise is (i.e. which problem to solve, and how). Then start to expand on it. \n",
    "\n",
    "1. Look at the 1D distributions of the two discriminating variables for the two species, and see how well you can separate them by eye. It seems somewhat possible, but certainly far from perfect... Once you consider the 2D distribution (scatter plot - to be uncommented by you!), then it is clear, that some cut along a line at an angle will work much better. This exercise is about finding that optimal line, and thus the perpendicular axis to project the data onto!\n",
    "\n",
    "2. Calculate from the data the mean, widths (std) and covariance of each discriminating variable (pair of variables for covariance) for each species, and put these into the matrices defined.\n",
    "\n",
    "3. From the inverted summed covariance matrices and vectors of means, calculate the two Fisher coefficients, and given these, calculate the Fisher discriminant for the two species in question, i.e. $ \\mathcal{F} = \\vec{w} \\cdot \\vec{x} = w_x \\cdot x + w_y \\cdot y $ for each point (x,y).\n",
    "\n",
    "4. What separation did you get, and is it notably better than what you obtain by eye? Also, do your weights make sense? I.e. are they comparable to the widths of the\n",
    "   corresponding variable? As a simple measure of how good the separation obtained is, we consider the \"distance\" $\\Delta$ between the two distributions as a measure of goodness:  \n",
    "   \n",
    "   $$\\Delta = \\frac{|\\mu_A-\\mu_B|}{\\sqrt{\\sigma_A^2+\\sigma_B^2}}$$\n",
    "   \n",
    "Compare the separation you get from each of the two 1D histograms of $x_0$ and $x_1$ with what you get from the Fisher discriminant, using the above formula. Of course the ultimate comparison should be done using ROC curves!"
   ]
  }
 ],
 "metadata": {
  "executable": "/usr/bin/env python",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "main_language": "python"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
